{"cells":[{"cell_type":"markdown","metadata":{"id":"ihsR_ew8QNCr"},"source":["#COVID-19 Prediction"]},{"cell_type":"markdown","metadata":{"id":"OAKefO7DSUwH"},"source":["Since late 2019, the world been in health crisis because of the COVID-19 pandemic. In fact, using Medical Imagery has proved to be efficient in detecting Covid-19 Infection. These Medical Imaging include: X-ray, CT-scans and Ultrasounds. The use of CT-scans is not only limited to the detection of COVID-19 cases, but they can also be used for other important tasks such quantifying the infection and monitoring the evolution of the disease, which can help in treatment and save the patient’s life. In this challenge, the participants will use a dataset labelled by two expert radiologists, who estimated the Covid-19 infection, to train and validate their approaches. In the testing phase, participants will test their approaches using a test dataset collected from various CT-scanners and recording settings."]},{"cell_type":"markdown","metadata":{"id":"C1SUlbVIQbXx"},"source":["- In your google drive, create a folder called \"DL_Project\"\n","- Place this .ipynb notebook in that folder\n","- Upload the training & val dataset zip in that folder\n","- Only have to do this once the very first time you are setting up project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ycwr5g60RUs","executionInfo":{"status":"ok","timestamp":1647901776670,"user_tz":240,"elapsed":15882,"user":{"displayName":"Zipei Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02593512087282554190"}},"outputId":"32b427f7-4762-480b-a974-c4e7bd4a6504"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP8GYguxqzLy"},"outputs":[],"source":["## if you haven't unzipped training data\n","#!unzip \"drive/MyDrive/DL_Project/Train.zip\" -d  \"drive/MyDrive/DL_Project/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOZCN2EusO5_"},"outputs":[],"source":["### if you haven't unzipped val data\n","#!unzip \"drive/MyDrive/DL_Project/Val Blind.zip\" -d  \"drive/MyDrive/DL_Project/\""]},{"cell_type":"markdown","metadata":{"id":"T58rAfGwOtiP"},"source":["##Challenge Dataset:\n","\n","The challenge has three sets: Train, Val, and Test, and we will only work with the Train set and Val set (unlabeled). \n","\n","The Train set is obtained from 132 CT-scans, from which 128 CT-scans has confirmed to have Covid-19 based on positive reverse transcription polymerase chain reaction (RT-PCR) and CT scan manifestations identified by two experienced thoracic radiologists. The rest four CT-scans have not any infection type (Healthy). The Val set is obtained from 57 CT-scans, from which 55 CT-scans has confirmed to have Covid-19 based on positive reverse transcription polymerase chain reaction (RT-PCR) and CT scan manifestations identified by two experienced thoracic radiologists. The rest two CT-scans have not any infection type (Healthy).\n","\n","The Train split has two files: Images (Slices) Folder and Labeling Folder ('.csv' file) that contains the labels for each Slice (Image)"]},{"cell_type":"markdown","metadata":{"id":"K8JKYqdFO4la"},"source":["    Train Set\n","    ├── Slices Folder \n","    │      ├ Image_0000.png\n","    │      ├ Image_0001.png\n","    │      ├ ...\n","    |      └ Image_3053.png\n","    ├── Labeling Folder\n","    │   └── Train.csv\n","    │         ├ Slice_Name        Covid-19_percentage  Subject    \n","    │         ├ Image_0000.png    0.0                  0\n","    │         ├ Image_0001.png    0.0                  0\n","    │         ├ ...               ...                  ..\n","    │         └ Image_3053.png    40.0\t               131\n","    └── "]},{"cell_type":"markdown","metadata":{"id":"CoiQf1kNPN0f"},"source":["The Validation split has one file contains the slices images, which will be used to predict the Covid-19 infection percentage. These prediction should be saved as '.csv' file and submitted to the Codalab (https://competitions.codalab.org/competitions/35575) to evaluation the performance:"]},{"cell_type":"markdown","metadata":{"id":"jii5kTiMPQHQ"},"source":["    Val Set\n","    ├── Slices Folder \n","    │      ├ Image_0000.png\n","    │      ├ Image_0001.png\n","    │      ├ ...\n","     |      └ Image_1300.png\n","    └── "]},{"cell_type":"markdown","metadata":{"id":"sGTBoEiXS0YP"},"source":["## Deliverables"]},{"cell_type":"markdown","metadata":{"id":"2cwK6XnLSt4E"},"source":["In the Validation phase, each team should submit the predictions of the validation data as 'predictions.csv' file, which contains the names of the slice images in the first column and the corresponding Covid-19 infection percentage estimation in the second column. **This file should be compressed as 'predictions.zip' file and submitted to CodaLab**.\n","\n","In order to have access to the leader board, you must do the following:\n","- Participation in the Competition: Each team should request to participate in the competition on the CodaLab platform (https://competitions.codalab.org/competitions/35575) with specifying the team name, members, emails and affiliations. The team informations can be send to: faresbougourzi@gmail.com"]},{"cell_type":"markdown","metadata":{"id":"KT60ecgITNED"},"source":["    predictions.zip\n","    ├── predictions.csv\n","    │      ├ Image_0000.png    Pr0            \n","    │      ├ Image_0001.png    Pr1             \n","    │      ├ ...               ...              \n","    │      └ Image_1300.png    Pr1300\t        \n","    └──   "]},{"cell_type":"markdown","metadata":{"id":"BoRb-o6NSiaA"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"YSx8S1ugSd2m"},"source":["The evaluation metrics are: **Mean Absolute Error (MAE), Pearson Correlation coefficient (PC) and Root Mean Square Error (RMSE)**. The most important Evaluation Criterion is the **MAE**. In the event of two or more competitors achieve the same MAE, the PC and the RMSE are considered as the tie-breaker."]},{"cell_type":"markdown","metadata":{"id":"gFBUy1lBQIM5"},"source":["## Custom Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TckWJ9O4sVJT"},"outputs":[],"source":["## preprocess data\n","## create custom data class\n","\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import cv2\n","from PIL import Image\n","import glob\n","\n","class CovidDataset(Dataset):\n","    \"\"\"Covid CT dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.label_data = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        ## you can apply custom transformation on the image for data augmentation\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.label_data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir, self.label_data.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n","        image = torchvision.transforms.functional.to_tensor(image)\n","        p = self.label_data.iloc[idx, 1]\n","        subject_num = self.label_data.iloc[idx, 2]\n","        sample = {'image': image, 'percentage': p, 'subject': subject_num, 'img_name':img_name}\n","\n","        # should be only applied on image, not percentage or subject #\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95hjpFoLaLWs"},"outputs":[],"source":["def transform(sample):\n","  image = sample['image']\n","  image = transforms.RandomHorizontalFlip(p=0.5)(image)\n","  return {'image': image, 'percentage': sample['percentage'], 'subject': sample['subject'], 'img_name':sample['img_name']}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0hPdBQZ3DLE"},"outputs":[],"source":["class CovidTestDataset(Dataset):\n","    \"\"\"Covid CT TEST dataset.\"\"\"\n","\n","    def __init__(self, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.image_list = []\n","        for filename in glob.glob(self.root_dir+\"/*.png\"): #assuming png\n","          self.image_list.append(filename)\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = self.image_list[idx]\n","        image = io.imread(img_name)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n","        image = torchvision.transforms.functional.to_tensor(image)\n","        sample = {'image': image, 'img_name':img_name}\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmV47ZBJy1Nv"},"outputs":[],"source":["import torchvision\n","covid_dataset_train_val = CovidDataset(csv_file='drive/MyDrive/DL_Project/Train.csv',\n","                                    root_dir='drive/MyDrive/DL_Project/Train'\n","                                    #, transform = transforms.Compose([\n","                                    #           Rescale(256),\n","                                    #           RandomCrop(224),\n","                                    #           ToTensor()\n","                                    #       ])\n","                                    )\n","\n","covid_dataset_test = CovidTestDataset(root_dir='drive/MyDrive/DL_Project/Val')"]},{"cell_type":"markdown","metadata":{"id":"ZV7i3Ov9REFZ"},"source":["##Split given train set to train & val set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiA6ycQE1wXu"},"outputs":[],"source":["## split given train set to train & val set\n","import random\n","print(len(covid_dataset_train_val))\n","\n","\"\"\"neg_indexs = []\n","for i in ind:\n","  sample = covid_dataset_train_val[i]\n","  if sample['percentage'] ==0:\n","    neg_indexs.append(i)\n","print(neg_indexs)  # save for later\"\"\"\n","\n","neg_indexs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 295, 311, 312, 313, 314, 315, 316, 317, 337, 338, 339, 340, 341, 342, 363, 364, 365, 366, 380, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 420, 518, 519, 520, 521, 522, 532, 533, 535, 536, 537, 538, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 555, 556, 557, 558, 559, 560, 561, 562, 563, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 608, 623, 624, 645, 646, 647, 648, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 794, 795, 796, 797, 798, 799, 806, 818, 842, 846, 847, 848, 849, 850, 851, 855, 859, 860, 861, 862, 863, 864, 865, 866, 899, 903, 904, 944, 945, 946, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1093, 1094, 1095, 1098, 1111, 1112, 1113, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1145, 1146, 1193, 1194, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1227, 1228, 1229, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1264, 1265, 1266, 1267, 1268, 1289, 1294, 1295, 1296, 1297, 1298, 1316, 1338, 1339, 1340, 1341, 1342, 1343, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1423, 1424, 1425, 1426, 1427, 1428, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1492, 1493, 1494, 1510, 1511, 1512, 1513, 1518, 1519, 1520, 1542, 1598, 1599, 1600, 1601, 1603, 1638, 1639, 1640, 1649, 1654, 1658, 1659, 1689, 1690, 1691, 1692, 1693, 1694, 1698, 1699, 1700, 1710, 1756, 1851, 1852, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1990, 1991, 1992, 2022, 2023, 2024, 2025, 2077, 2078, 2079, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2177, 2178, 2179, 2180, 2181, 2206, 2207, 2208, 2237, 2238, 2239, 2240, 2241, 2243, 2244, 2408, 2409, 2410, 2432, 2433, 2434, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2538, 2606, 2608, 2609, 2610, 2621, 2622, 2651, 2653, 2655, 2656, 2663, 2670, 2671, 2672, 2681, 2682, 2707, 2708, 2709, 2711, 2712, 2719, 2720, 2723, 2724, 2743, 2744, 2745, 2746, 2749, 2754, 2757, 2758, 2789, 2790, 2791, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2856, 2857, 2889, 2890, 2891, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2902, 2903, 2904, 2905, 2906, 2945, 2946, 2947, 2948, 2949, 2964, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2993, 2994, 2995, 3006, 3007, 3008, 3011, 3012, 3015, 3025, 3026, 3027, 3028, 3029, 3030, 3034, 3035, 3036, 3044]\n","\n","pos_indexs = [i for i in range(len(covid_dataset_train_val)) if i not in neg_indexs]\n","\n","random.shuffle(pos_indexs)\n","random.shuffle(neg_indexs)\n","\n","train_sampler = pos_indexs[:int(len(pos_indexs)*0.8)] + neg_indexs[:int(len(neg_indexs)*0.8)]\n","valid_sampler = [i for i in range(len(covid_dataset_train_val)) if i not in train_sampler]\n","\n","train_dataset = torch.utils.data.Subset(covid_dataset_train_val, train_sampler)\n","val_dataset = torch.utils.data.Subset(covid_dataset_train_val, valid_sampler)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)\n","\n","## test set\n","test_dataloader = DataLoader(covid_dataset_test, batch_size=4, shuffle=False, num_workers=0)\n","\n","print(len(train_sampler))\n","print(len(valid_sampler))\n"]},{"cell_type":"markdown","metadata":{"id":"OClBrnNnRK_t"},"source":["## Import pretrained models library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIQiUJcyvJ1y"},"outputs":[],"source":["## use imagenet pretrained model\n","## let's start with resnet34\n","!pip install pretrainedmodels"]},{"cell_type":"markdown","metadata":{"id":"NJTMiALSRRHC"},"source":["## Use CUDA to speed up training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BT-6cVxs_iT"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"wqPiUx1XRbIL"},"source":["## Create Baseline Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DIFuG0hvZn7"},"outputs":[],"source":["import pretrainedmodels\n","#For model building\n","import torch\n","from torch import nn, optim\n","import torchvision\n","from torch.nn import functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super(CNN, self).__init__()\n","        if pretrained is True:\n","            self.model1 = pretrainedmodels.__dict__[\"resnet101\"](pretrained=\"imagenet\")\n","            self.model2 = pretrainedmodels.__dict__[\"resnet50\"](pretrained=\"imagenet\")\n","        else:\n","            self.model = pretrainedmodels.__dict__[\"resnet101\"](pretrained=None)\n","        self.conv1 = nn.Conv2d(4096, 256, 3, stride=1, padding =0)\n","        self.normal1 = nn.InstanceNorm2d(256)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features=(256*14*14), out_features=4096),\n","            nn.ReLU(),\n","            nn.Linear(in_features=4096, out_features=1),\n","        )\n","        self.conv3 = nn.Conv2d(32, 8, 3, stride=1, padding =0)\n","        \n","    def forward(self, x):\n","        bs, _, _, _ = x.shape\n","        x1 = self.model1.features(x)\n","        x2 = self.model2.features(x)\n","        x = torch.cat((x1,x2),1)\n","        #print(x.size())\n","        x = self.conv1(x)\n","        x = self.normal1(x)\n","        label = self.classifier(x.reshape(bs, -1))\n","        return label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufc5DuRU_8VO"},"outputs":[],"source":["#Setting model and moving to device\n","model_CNN = CNN(True).to(device)\n","\n","criterion = nn.L1Loss()\n","optimizer = optim.Adam(model_CNN.parameters(), lr=0.001, weight_decay=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O42uOYtUAK2D"},"outputs":[],"source":["def train_model(model, criterion, optimizer, n_epochs=60):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf\n","\n","    for epoch in range(1, n_epochs):\n","      train = [0,1,2,3]\n","      train_loss = 0.0\n","      valid_loss = 0.0\n","      # train the model #\n","      model.train()\n","      for batch_idx, sample_batched in enumerate(train_dataloader):\n","            # importing data and moving to GPU\n","            image, label = sample_batched['image'].to(device), sample_batched['percentage'].to(device)\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","            output=model(image).reshape(-1)\n","            # calculate loss\n","            loss = criterion((output).type(torch.FloatTensor).to(device), label.type(torch.FloatTensor).to(device))\n","            # back prop\n","            loss.backward()\n","            # grad\n","            optimizer.step()\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            if batch_idx % 100 == 0:\n","                print('Epoch %d, Batch %d loss: %.6f' %\n","                  (epoch, batch_idx + 1, train_loss))\n","      # validate the model #\n","      model.eval()\n","      for batch_idx, sample_batched in enumerate(val_dataloader):\n","            image, label = sample_batched['image'].to(device), sample_batched['percentage'].to(device)\n","            output = model(image).reshape(-1)\n","            # calculate loss\n","            loss=criterion((output).type(torch.FloatTensor).to(device), label.type(torch.FloatTensor).to(device))\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","            \n","      # print training/validation statistics \n","      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss, valid_loss))\n","      \n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss < valid_loss_min:\n","          torch.save(model.state_dict(), 'drive/MyDrive/DL_Project/model.pt')\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min,\n","          valid_loss))\n","          valid_loss_min = valid_loss\n","    # return trained model\n","    torch.save(model.state_dict(), 'drive/MyDrive/DL_Project/model_last.pt')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRcDzZQnAScu"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","#model_CNN.load_state_dict(torch.load('drive/MyDrive/DL_Project/model.pt'))\n","model_conv=train_model(model_CNN, criterion, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"IJePsT4QUDdh"},"source":["## Save result on Actual Val Data to csv File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaozGKNc4V-o"},"outputs":[],"source":["#torch.save(model_conv.state_dict(), 'drive/MyDrive/DL_Project/model_01.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxAQKYA4UJvj"},"outputs":[],"source":["df = pd.DataFrame(columns=['image_name','output'])\n","for batch_idx, sample_batched in enumerate(test_dataloader):\n","    image= sample_batched['image'].to(device)\n","    img_name= sample_batched['img_name']\n","    output = model_conv(image).type(torch.LongTensor).reshape(-1)\n","    img_name = np.array(img_name).reshape(output.shape[0],1)\n","    o = output.cpu().data.numpy().reshape(output.shape[0],1)\n","    a = np.concatenate((img_name,o),axis=1)\n","    df = df.append(pd.DataFrame(a, columns=df.columns), ignore_index=True)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxgoKACBUL_7"},"outputs":[],"source":["#Extracting image name from the image path\n","df['image_name']=df['image_name'].str.split(\"/\").str[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_3kthO_X1Ak"},"outputs":[],"source":["df.sort_values(df.columns[0], axis=0, inplace=True)\n","df.to_csv('drive/MyDrive/DL_Project/predictions.csv', index=False, header=False)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"iMJCiCG-M1Ut"},"source":["## Points to note:\n","\n","General Rules: Participants should estimate the percentage of Covid-19 infection from each slice using Machine Learning. **Only ImageNet's pre-trained models and Lung Nodule Segmentation models are allowed**. The use of external data or other pre-trained models is not allowed. The models must be trained using the training data and evaluated using the validation data.\n","\n","##Things you can try:\n","\n","- The dataset is not as big and especially negative covid images are very few.\n","It would be better to use k-fold cross validation rather than conventional splitting.\n","- How would you split image splices from the same patient subject into train & val set?\n","- How would the information on the patient (subject #) help your prediction?\n","- What kind of preprocessing/data augmentation method help your model? What kind of methods would actually make your model perform worse?\n","- What other different CNN architectures could you explore to acheive lower MAE?\n","- Explore different optimizers, loss combinations, etc\n","- Explore different regularization methods\n","\n","This is no way an exhaustive list. You might get a better idea by reading relevant research papers. Good luck!"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"covid19_code_submimssion","provenance":[{"file_id":"1WfdFv-L-t7aYJ1sP9xWEz8tVXCCK4apC","timestamp":1647903054327},{"file_id":"1DBrq_8uQ4hDy-KkIleNtC4-13lf04Cdg","timestamp":1645466631715}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}